{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04c4346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "#import harp\n",
    "from netCDF4 import Dataset\n",
    "import os\n",
    "from os.path import join\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c3331a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tropospheric_NO2_column_number_density', 'latitude', 'longitude', 'time'])\n",
      "lat [ 8.005  8.015  8.025 ... 36.965 36.975 36.985]\n",
      "lon [68.005 68.015 68.025 ... 96.965 96.975 96.985]\n",
      "no2 [[[-- -- -- ... -- -- --]\n",
      "  [-- -- -- ... -- -- --]\n",
      "  [-- -- -- ... -- -- --]\n",
      "  ...\n",
      "  [-- -- -- ... 0.8934862801990139 0.8934862801997281 0.9101298225528585]\n",
      "  [-- -- -- ... 0.893486280199371 0.8934862801990933 1.0535357590655705]\n",
      "  [-- -- -- ... 0.8934862801990139 0.9001095148394141 1.2166691595353685]]\n",
      "\n",
      " [[-- -- -- ... 0.39692594058520886 0.17109398945177076\n",
      "   0.17109398945196072]\n",
      "  [-- -- -- ... 0.3062241037452048 0.17109398945177076\n",
      "   0.17109398945196072]\n",
      "  [-- -- -- ... 0.2151974610707862 0.1710944610101049\n",
      "   0.17109398945196072]\n",
      "  ...\n",
      "  [1.326423744628536 1.3264237446289484 1.326423744630009 ...\n",
      "   0.774007264747734 0.7740072647483527 0.7740072647474933]\n",
      "  [1.3264237446290663 1.3264237446294787 1.3264237446290663 ...\n",
      "   0.7740072647480433 0.7740072647478027 0.7740072647478027]\n",
      "  [1.326423744628536 1.3264237446289484 1.326423744630009 ...\n",
      "   0.774007264747734 0.7740072647483527 0.7740072647474933]]\n",
      "\n",
      " [[0.22268869613197134 1.2725160501209125 1.2808913570602873 ... -- --\n",
      "   --]\n",
      "  [0.4574490023876295 1.2808913570606857 1.2808913570602873 ... -- -- --]\n",
      "  [0.7005846155806393 1.2808913570606857 1.2808913570602873 ... -- -- --]\n",
      "  ...\n",
      "  [1.0810965012178047 1.0821623616071796 1.0836027066130574 ... -- -- --]\n",
      "  [1.2735509481909246 1.2739666518807262 1.2739666518803303 ... -- -- --]\n",
      "  [1.273966651879821 1.273966651880217 1.2739666518812354 ... -- -- --]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-- -- -- ... -0.8636380787493113 -0.8635893468020802\n",
      "   -0.7542545401404619]\n",
      "  [-- -- -- ... -0.8636380787493113 -0.8447483360009526\n",
      "   -0.7333142837739279]\n",
      "  [-- -- -- ... -0.8636380787493113 -0.8106166482809066\n",
      "   -0.7333142837739279]\n",
      "  ...\n",
      "  [-- -- -- ... 0.5173599339733155 0.562630776453461 0.6540101698348554]\n",
      "  [-- -- -- ... 0.5173599339735222 0.6129134237576812 0.6540101698351168]\n",
      "  [-- -- -- ... 0.808655114487463 0.7218213694966261 0.6603489054121998]]\n",
      "\n",
      " [[-- -- -- ... -- -- --]\n",
      "  [-- -- -- ... -- -- --]\n",
      "  [-- -- -- ... -- -- --]\n",
      "  ...\n",
      "  [0.37916551021687267 0.39157397306495334 0.3983606856269202 ... -- --\n",
      "   --]\n",
      "  [0.3809272147521222 0.3978942868295798 0.3983606856266371 ... -- -- --]\n",
      "  [0.5953551460653226 0.4115024123152124 0.3983606856269202 ... -- -- --]]\n",
      "\n",
      " [[0.6530097764151894 0.6530097764146675 0.6530097764144644 ... -- -- --]\n",
      "  [0.6530097764151894 0.6530097764146675 0.6530097764144644 ... -- -- --]\n",
      "  [0.6530097764151894 0.6530097764146675 0.6530097764144644 ... -- -- --]\n",
      "  ...\n",
      "  [-- -- -- ... -- -- --]\n",
      "  [-- -- -- ... -- -- --]\n",
      "  [-- -- -- ... -- -- --]]]\n",
      "time [     0   6089  12178  91339  97429 176590 182679 261840 267930 347093\n",
      " 353181 359271]\n"
     ]
    }
   ],
   "source": [
    "L3_data1=Dataset(r\"5days_combined.nc\")\n",
    "print(L3_data1.variables.keys())\n",
    "lat=L3_data1.variables['latitude'][:]\n",
    "lon=L3_data1.variables['longitude'][:]\n",
    "time_data=L3_data1.variables['time'][:]\n",
    "no2=L3_data1.variables['tropospheric_NO2_column_number_density'][0:,:,:]\n",
    "print(\"lat\", lat)\n",
    "print(\"lon\", lon)\n",
    "print(\"no2\", no2)\n",
    "print(\"time\",time_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef9ff8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(latitude_input,longitude_input,date):\n",
    "    \n",
    "\n",
    "   \n",
    "    predict_days=abs((datetime.strptime('2024-09-05',\"%Y-%m-%d\")-datetime.strptime(date,\"%Y-%m-%d\")).days)\n",
    "    st.write(\"--------predict days-----:\",predict_days)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    sq_diff_lat=(lat-latitude_input)**2\n",
    "    sq_diff_lon=(lon-longitude_input)**2\n",
    "    \n",
    "    min_index_lat=sq_diff_lat.argmin()\n",
    "    min_index_lon=sq_diff_lon.argmin()\n",
    "    \n",
    "    start_date=L3_data1.variables['time'].units[14:24]\n",
    "    end_date=L3_data1.variables['time'].units[14:18]+'-09-05'\n",
    "    \n",
    "    date_range=pd.date_range(start=start_date,end=end_date)\n",
    "    st.success(date_range)   \n",
    "    df=pd.DataFrame(0,columns=['NO2'],index=date_range)\n",
    "    print(df)\n",
    "    dt=np.arange(0,5)\n",
    "\n",
    "    print(dt)\n",
    "    \n",
    "    for i in dt:\n",
    "        df.iloc[i]=no2[i,min_index_lat,min_index_lon]\n",
    "        \n",
    "    df.to_csv(r\"5days_combined.csv\")\n",
    "    #print(os.path.getsize(r\"C:\\Users\\HP\\.spyder-py3\\data\\days_combined.csv\"))\n",
    "    \n",
    "    ##############################################\n",
    "    #            PREDICTION MODULE               #\n",
    "    ##############################################\n",
    "    ### Data Collection\n",
    "    data_frame=pd.read_csv(r\"5days_combined.csv\")\n",
    "    df1=data_frame.reset_index()['NO2']\n",
    "    df1 = df1.fillna(0)\n",
    "    print(df1)\n",
    "    ### LSTM are sensitive to the scale of the data. so we apply MinMax scaler \n",
    "\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    df1=scaler.fit_transform(np.array(df1).reshape(-1,1))\n",
    "    \n",
    "    ##splitting dataset into train and test split\n",
    "\n",
    "\n",
    "    training_size=int(len(df1)*0.90)\n",
    "    test_size=len(df1)-training_size\n",
    "    train_data,test_data=df1[0:training_size,:],df1[test_size:len(df1),:]\n",
    "    print(train_data)\n",
    "    print(test_data)\n",
    "       \n",
    "    \n",
    "    # convert an array of values into a dataset matrix\n",
    "    def create_dataset(dataset, time_step=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-time_step-1):         \n",
    "            a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 \n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + time_step, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "    \n",
    "    # reshape into X=t,t+1,t+2,t+3 and Y=t+4\n",
    "    time_step = 1\n",
    "    X_train, y_train = create_dataset(train_data, time_step)\n",
    "    X_test, ytest = create_dataset(test_data, time_step)\n",
    "\n",
    "    print(X_train.shape), print(y_train.shape)\n",
    "    print(X_test.shape), print(ytest.shape)\n",
    "\n",
    "    \n",
    "    # print(X_test.shape), print(ytest.shape)\n",
    "  \n",
    "    # reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "    X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "    print(X_train)\n",
    "    \n",
    "    ### Create the Stacked LSTM model\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(50,return_sequences=True,input_shape=(1,1)))\n",
    "    model.add(LSTM(50,return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=25,batch_size=2,verbose=1)\n",
    "    \n",
    "\n",
    "    ### Lets Do the prediction and check performance metrics\n",
    "    train_predict=model.predict(X_train)\n",
    "    test_predict=model.predict(X_test)\n",
    "    \n",
    "    ##Transformback to original form\n",
    "    train_predict=scaler.inverse_transform(train_predict)\n",
    "    test_predict=scaler.inverse_transform(test_predict)\n",
    "    \n",
    "    ### Calculate RMSE performance metrics\n",
    "    \n",
    "    math.sqrt(mean_squared_error(y_train,train_predict))\n",
    "    \n",
    "    ### Test Data RMSE\n",
    "    math.sqrt(mean_squared_error(ytest,test_predict))\n",
    "    \n",
    "    ### Plotting \n",
    "    # shift train predictions for plotting\n",
    "    look_back=1\n",
    "    trainPredictPlot = np.empty_like(df1)\n",
    "    trainPredictPlot[:, :] = np.nan\n",
    "    trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "    # shift test predictions for plotting\n",
    "    testPredictPlot = np.empty_like(df1)\n",
    "    testPredictPlot[:, :] = np.nan\n",
    "    testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict\n",
    "      \n",
    "    x_input=test_data[len(test_data)-1:].reshape(1,-1)\n",
    "    \n",
    "    temp_input=list(x_input)\n",
    "    temp_input=temp_input[0].tolist()\n",
    "    \n",
    "    # demonstrate prediction for next days\n",
    "    \n",
    "    lst_output=[]\n",
    "    n_steps=1\n",
    "    i=0\n",
    "    while(i<predict_days):\n",
    "        \n",
    "        if(len(temp_input)>1):\n",
    "            #print(temp_input)\n",
    "            x_input=np.array(temp_input[1:])\n",
    "            print(\"{} day input {}\".format(i,x_input))\n",
    "            x_input=x_input.reshape(1,-1)\n",
    "            x_input = x_input.reshape((1, n_steps, 1))\n",
    "            #print(x_input)\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            print(\"{} day output {}\".format(i,yhat))\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            temp_input=temp_input[1:]\n",
    "            #print(temp_input)\n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "        else:\n",
    "            x_input = x_input.reshape((1, n_steps,1))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            print(yhat[0])\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            print(len(temp_input))\n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "        \n",
    "    \n",
    "    print(lst_output)\n",
    "        \n",
    "    # st.write(df3)\n",
    "    no2_output=pd.DataFrame(scaler.inverse_transform(lst_output),columns=['NO2 Concentration ðŸ­'])\n",
    "    print(no2_output)\n",
    "    output= (no2_output.at[predict_days-1,'NO2 Concentration ðŸ­'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b9755e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 02:36:46.253 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-29 02:36:46.255 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-29 02:36:46.256 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-29 02:36:46.257 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-29 02:36:46.260 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-29 02:36:46.262 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "C:\\Users\\PRAMILA\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\blocks.py:1067: UserWarning: Warning: converting a masked element to nan.\n",
      "  values[indexer] = casted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            NO2\n",
      "2024-09-01    0\n",
      "2024-09-02    0\n",
      "2024-09-03    0\n",
      "2024-09-04    0\n",
      "2024-09-05    0\n",
      "[0 1 2 3 4]\n",
      "0    0.000000\n",
      "1    0.000000\n",
      "2    0.255614\n",
      "3    0.000000\n",
      "4    0.342948\n",
      "Name: NO2, dtype: float64\n",
      "[[0.       ]\n",
      " [0.       ]\n",
      " [0.7453447]\n",
      " [0.       ]]\n",
      "[[0.       ]\n",
      " [0.7453447]\n",
      " [0.       ]\n",
      " [1.       ]]\n",
      "(2, 1)\n",
      "(2,)\n",
      "(2, 1)\n",
      "(2,)\n",
      "[[[0.]]\n",
      "\n",
      " [[0.]]]\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_39 (LSTM)              (None, 1, 50)             10400     \n",
      "                                                                 \n",
      " lstm_40 (LSTM)              (None, 1, 50)             20200     \n",
      "                                                                 \n",
      " lstm_41 (LSTM)              (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50851 (198.64 KB)\n",
      "Trainable params: 50851 (198.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.2778 - val_loss: 0.2752\n",
      "Epoch 2/25\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2752 - val_loss: 0.2726\n",
      "Epoch 3/25\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2726 - val_loss: 0.2699\n",
      "Epoch 4/25\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2699 - val_loss: 0.2673\n",
      "Epoch 5/25\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2672 - val_loss: 0.2645\n",
      "Epoch 6/25\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2645 - val_loss: 0.2618\n",
      "Epoch 7/25\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2618 - val_loss: 0.2590\n",
      "Epoch 8/25\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2590 - val_loss: 0.2561\n",
      "Epoch 9/25\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2561 - val_loss: 0.2532\n",
      "Epoch 10/25\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2532 - val_loss: 0.2503\n",
      "Epoch 11/25\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2502 - val_loss: 0.2473\n",
      "Epoch 12/25\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2472 - val_loss: 0.2442\n",
      "Epoch 13/25\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2441 - val_loss: 0.2411\n",
      "Epoch 14/25\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2410 - val_loss: 0.2379\n",
      "Epoch 15/25\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2378 - val_loss: 0.2347\n",
      "Epoch 16/25\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2346 - val_loss: 0.2314\n",
      "Epoch 17/25\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2313 - val_loss: 0.2281\n",
      "Epoch 18/25\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2279 - val_loss: 0.2247\n",
      "Epoch 19/25\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2245 - val_loss: 0.2212\n",
      "Epoch 20/25\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2210 - val_loss: 0.2177\n",
      "Epoch 21/25\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2174 - val_loss: 0.2142\n",
      "Epoch 22/25\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2138 - val_loss: 0.2106\n",
      "Epoch 23/25\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2101 - val_loss: 0.2069\n",
      "Epoch 24/25\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2064 - val_loss: 0.2032\n",
      "Epoch 25/25\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2026 - val_loss: 0.1995\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 59ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,1) into shape (0,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m longitude_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m19.3\u001b[39m\n\u001b[0;32m      3\u001b[0m date\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-09-09\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatitude_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlongitude_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[73], line 118\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(latitude_input, longitude_input, date)\u001b[0m\n\u001b[0;32m    116\u001b[0m testPredictPlot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty_like(df1)\n\u001b[0;32m    117\u001b[0m testPredictPlot[:, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m--> 118\u001b[0m \u001b[43mtestPredictPlot\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlook_back\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m test_predict\n\u001b[0;32m    120\u001b[0m x_input\u001b[38;5;241m=\u001b[39mtest_data[\u001b[38;5;28mlen\u001b[39m(test_data)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    122\u001b[0m temp_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(x_input)\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (2,1) into shape (0,1)"
     ]
    }
   ],
   "source": [
    "\n",
    "latitude_input=19.3\n",
    "longitude_input=19.3\n",
    "date= \"2024-09-09\"\n",
    "result = predict(latitude_input,longitude_input,date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f5c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c61d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea142ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48198169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ab030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13cd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394d162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3513a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
